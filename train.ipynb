{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EasyDict' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b13b8997eab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-b13b8997eab2>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepsupervision\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s_%s_wDS'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'EasyDict' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from skimage.io import imread\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from dataset import Dataset\n",
    "\n",
    "from metrics import dice_coef, batch_iou, mean_iou, iou_score\n",
    "import losses\n",
    "from utils import str2bool, count_params\n",
    "import pandas as pd\n",
    "import FCN\n",
    "import easydict\n",
    "\n",
    "arch_names = list(FCN.__dict__.keys())\n",
    "loss_names = list(losses.__dict__.keys())\n",
    "loss_names.append('BCEWithLogitsLoss')\n",
    "\n",
    "'''\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--name', default=None,\n",
    "                        help='model name: (default: arch+timestamp)')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='FCN8s',\n",
    "                        choices=arch_names,\n",
    "                        help='model architecture: ' +\n",
    "                            ' | '.join(arch_names) +\n",
    "                            ' (default: NestedUNet)')\n",
    "    parser.add_argument('--deepsupervision', default=False, type=str2bool)\n",
    "    parser.add_argument('--dataset', default=\"jiu0Monkey\",\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--input-channels', default=4, type=int,\n",
    "                        help='input channels')\n",
    "    parser.add_argument('--image-ext', default='png',\n",
    "                        help='image file extension')\n",
    "    parser.add_argument('--mask-ext', default='png',\n",
    "                        help='mask file extension')\n",
    "    parser.add_argument('--aug', default=False, type=str2bool)\n",
    "    parser.add_argument('--loss', default='BCEDiceLoss',\n",
    "                        choices=loss_names,\n",
    "                        help='loss: ' +\n",
    "                            ' | '.join(loss_names) +\n",
    "                            ' (default: BCEDiceLoss)')\n",
    "    parser.add_argument('--epochs', default=10000, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--early-stop', default=20, type=int,\n",
    "                        metavar='N', help='early stopping (default: 20)')\n",
    "    parser.add_argument('-b', '--batch-size', default=18, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 16)')\n",
    "    parser.add_argument('--optimizer', default='Adam',\n",
    "                        choices=['Adam', 'SGD'],\n",
    "                        help='loss: ' +\n",
    "                            ' | '.join(['Adam', 'SGD']) +\n",
    "                            ' (default: Adam)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=3e-4, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', default=1e-4, type=float,\n",
    "                        help='weight decay')\n",
    "    parser.add_argument('--nesterov', default=False, type=str2bool,\n",
    "                        help='nesterov')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "'''\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    \"--name\":None,\n",
    "    \"--arch\":\"FCN8s\",\n",
    "    \"--deepsupervision\":False,\n",
    "    \"--dataset\":\"jiu0Monkey\",\n",
    "    \"--input-channels\":4,\n",
    "    \"--image-ext\":\"png\",\n",
    "    \"--mask-ext\":\"png\",\n",
    "    \"--aug\":False,\n",
    "    \"--loss\":\"BCEDiceLoss\",\n",
    "    \"--epochs\":10000,\n",
    "    \"--early-stop\":20,\n",
    "    \"-b\":18,\n",
    "    \"--optimizer\":\"Adam\",\n",
    "    \"--lr\":3e-4,\n",
    "    \"--momentum\":0.9,\n",
    "    \"--weight-decay\":1e-4,\n",
    "    \"--nesterov\":False\n",
    "})\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train(args, train_loader, model, criterion, optimizer, epoch, scheduler=None):\n",
    "    losses = AverageMeter()\n",
    "    ious = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (input, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        if args.deepsupervision:\n",
    "            outputs = model(input)\n",
    "            loss = 0\n",
    "            for output in outputs:\n",
    "                loss += criterion(output, target)\n",
    "            loss /= len(outputs)\n",
    "            iou = iou_score(outputs[-1], target)\n",
    "        else:\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou = iou_score(output, target)\n",
    "\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        ious.update(iou, input.size(0))\n",
    "\n",
    "        # compute gradient and do optimizing step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    log = OrderedDict([\n",
    "        ('loss', losses.avg),\n",
    "        ('iou', ious.avg),\n",
    "    ])\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def validate(args, val_loader, model, criterion):\n",
    "    losses = AverageMeter()\n",
    "    ious = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            if args.deepsupervision:\n",
    "                outputs = model(input)\n",
    "                loss = 0\n",
    "                for output in outputs:\n",
    "                    loss += criterion(output, target)\n",
    "                loss /= len(outputs)\n",
    "                iou = iou_score(outputs[-1], target)\n",
    "            else:\n",
    "                output = model(input)\n",
    "                loss = criterion(output, target)\n",
    "                iou = iou_score(output, target)\n",
    "\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            ious.update(iou, input.size(0))\n",
    "\n",
    "    log = OrderedDict([\n",
    "        ('loss', losses.avg),\n",
    "        ('iou', ious.avg),\n",
    "    ])\n",
    "\n",
    "    return log\n",
    "\n",
    "\n",
    "def main():\n",
    "    #args = parse_args()\n",
    "    args = easydict.EasyDict()\n",
    "    #args.dataset = \"datasets\"\n",
    "\n",
    "\n",
    "    if args.name is None:\n",
    "        if args.deepsupervision:\n",
    "            args.name = '%s_%s_wDS' %(args.dataset, args.arch)\n",
    "        else:\n",
    "            args.name = '%s_%s_woDS' %(args.dataset, args.arch)\n",
    "    if not os.path.exists('models/%s' %args.name):\n",
    "        os.makedirs('models/%s' %args.name)\n",
    "\n",
    "    print('Config -----')\n",
    "    for arg in vars(args):\n",
    "        print('%s: %s' %(arg, getattr(args, arg)))\n",
    "    print('------------')\n",
    "\n",
    "    with open('models/%s/args.txt' %args.name, 'w') as f:\n",
    "        for arg in vars(args):\n",
    "            print('%s: %s' %(arg, getattr(args, arg)), file=f)\n",
    "\n",
    "    joblib.dump(args, 'models/%s/args.pkl' %args.name)\n",
    "\n",
    "    # define loss function (criterion)\n",
    "    if args.loss == 'BCEWithLogitsLoss':\n",
    "        criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "    else:\n",
    "        criterion = losses.__dict__[args.loss]().cuda()\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    img_paths = glob(r'C:\\Users\\user\\Desktop\\Merofine\\BraTS2Dpreprocessing-master\\testImage\\*')\n",
    "    mask_paths = glob(r'C:\\Users\\user\\Desktop\\Merofine\\BraTS2Dpreprocessing-master\\trainMask\\*')\n",
    "\n",
    "    train_img_paths, val_img_paths, train_mask_paths, val_mask_paths = \\\n",
    "        train_test_split(img_paths, mask_paths, test_size=0.2, random_state=41)\n",
    "    print(\"train_num:%s\"%str(len(train_img_paths)))\n",
    "    print(\"val_num:%s\"%str(len(val_img_paths)))\n",
    "\n",
    "\n",
    "    # create model\n",
    "    print(\"=> creating model %s\" %args.arch)\n",
    "    model = FCN.__dict__[args.arch](args)\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    print(count_params(model))\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)\n",
    "    elif args.optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr,\n",
    "            momentum=args.momentum, weight_decay=args.weight_decay, nesterov=args.nesterov)\n",
    "\n",
    "    train_dataset = Dataset(args, train_img_paths, train_mask_paths, args.aug)\n",
    "    val_dataset = Dataset(args, val_img_paths, val_mask_paths)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)\n",
    "\n",
    "    log = pd.DataFrame(index=[], columns=[\n",
    "        'epoch', 'lr', 'loss', 'iou', 'val_loss', 'val_iou'\n",
    "    ])\n",
    "\n",
    "    best_iou = 0\n",
    "    trigger = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        print('Epoch [%d/%d]' %(epoch, args.epochs))\n",
    "\n",
    "        # train for one epoch\n",
    "        train_log = train(args, train_loader, model, criterion, optimizer, epoch)\n",
    "        # evaluate on validation set\n",
    "        val_log = validate(args, val_loader, model, criterion)\n",
    "\n",
    "        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n",
    "            %(train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n",
    "\n",
    "        tmp = pd.Series([\n",
    "            epoch,\n",
    "            args.lr,\n",
    "            train_log['loss'],\n",
    "            train_log['iou'],\n",
    "            val_log['loss'],\n",
    "            val_log['iou'],\n",
    "        ], index=['epoch', 'lr', 'loss', 'iou', 'val_loss', 'val_iou'])\n",
    "\n",
    "        log = log.append(tmp, ignore_index=True)\n",
    "        log.to_csv('models/%s/log.csv' %args.name, index=False)\n",
    "\n",
    "        trigger += 1\n",
    "\n",
    "        if val_log['iou'] > best_iou:\n",
    "            torch.save(model.state_dict(), 'models/%s/model.pth' %args.name)\n",
    "            best_iou = val_log['iou']\n",
    "            print(\"=> saved best model\")\n",
    "            trigger = 0\n",
    "\n",
    "        # early stopping\n",
    "        if not args.early_stop is None:\n",
    "            if trigger >= args.early_stop:\n",
    "                print(\"=> early stopping\")\n",
    "                break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
